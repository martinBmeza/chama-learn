{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b706017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 14:32:37.284846: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfkl\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e55ede",
   "metadata": {},
   "source": [
    "Cargo el archivo de texto con el que voy a trabajar. Faltan algunas tareas por realizar, como: \n",
    "- Limpiar algunas palabras que no son parte de las letras (como la palabra estribillo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ff8c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niangapiri silvestre luz del arenal \n",
      "tibio rubor del verde gris de la niniez \n",
      "\n",
      "\n",
      "mi resplandor mi atardecer mi taragi \n",
      "y el cielo aqui de tu mirar niangapiri \n",
      "\n",
      "\n",
      "anduve mirando el pueblo buscandome \n",
      "y fluyes como mi sangre de chamame \n",
      "enciende tu pie desnudo sobre mi piel \n",
      "un rio de primavera ternura y miel \n",
      "a orillas de tu sonrisa yo soy feliz \n",
      "no quiero partir de nuevo niangapiri \n",
      "\n",
      "\n",
      "la luna se ardio en el rio niangapiri \n",
      "y vuela a lo mas profundo de tu vivir \n",
      "alli donde es todo aroma va mi canci\n"
     ]
    }
   ],
   "source": [
    "#filepath = '../data/folclore-web/clean_dataset.txt'\n",
    "filepath = '../data/chamame-web/clean_dataset.txt'\n",
    "f = open(filepath)\n",
    "folclore_dataset = f.read()\n",
    "print(folclore_dataset[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d2aef",
   "metadata": {},
   "source": [
    "Ahora codifico a cada caracter como un numero entero. Para eso se usa el *Tokenizer*. Esto lo que hace es primero identificar todos los caracteres presentes, y luego asignarle un codigo o Token a cada uno. Empieza desde 1 hasta el numero de caracteres diferentes que encuentre en el dataset. Se debe tener en cuenta que el tokenizer convierte todo el texto en lowercase. Se puede indicarle lower=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d2382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True) #default es word level encoding! \n",
    "tokenizer.fit_on_texts(folclore_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7784dd",
   "metadata": {},
   "source": [
    "Con esto, el tokenizer puede codificar texto en forma de tokes o IDs, y viceversa. Ademas, nos dice cuantos caracteres diferentes hay, y el numero de caracteres en el texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07cb5f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 4, 10, 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Hola!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437f4082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['j o i a']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[23, 4, 9, 3, 45]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a49d219e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 1,\n",
       " 'e': 2,\n",
       " 'a': 3,\n",
       " 'o': 4,\n",
       " 'n': 5,\n",
       " 'r': 6,\n",
       " 's': 7,\n",
       " '\\n': 8,\n",
       " 'i': 9,\n",
       " 'l': 10,\n",
       " 'u': 11,\n",
       " 'd': 12,\n",
       " 't': 13,\n",
       " 'c': 14,\n",
       " 'm': 15,\n",
       " 'p': 16,\n",
       " 'v': 17,\n",
       " 'y': 18,\n",
       " 'q': 19,\n",
       " 'g': 20,\n",
       " 'b': 21,\n",
       " 'h': 22,\n",
       " 'j': 23,\n",
       " 'z': 24,\n",
       " 'f': 25,\n",
       " 'k': 26,\n",
       " 'x': 27}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "432ba83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de caracteres diferentes:  27\n",
      "Cantidad de caracteres en el dataset:  88204\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # Cantiad de caracteres distintos\n",
    "print('Cantidad de caracteres diferentes: ', max_id)\n",
    "dataset_size = tokenizer.document_count\n",
    "print('Cantidad de caracteres en el dataset: ',dataset_size) # Cantidad de caracteres en el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ba147",
   "metadata": {},
   "source": [
    "A fines practicos, voy a codificar todo el dataset para que cada caracter este representado por su ID. Voy a restar uno a la codificación para ir de 0 a 81 en lugar de 1 a 82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9769610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88204"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = np.array(tokenizer.texts_to_sequences(folclore_dataset)) - 1\n",
    "len(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78814d9",
   "metadata": {},
   "source": [
    "divido los datos para entrenamiento, validacion y prueba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10ffafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 14:32:47.990551: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-13 14:32:48.023730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.024022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-11-13 14:32:48.024040: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-11-13 14:32:48.026949: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-11-13 14:32:48.026978: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-11-13 14:32:48.028388: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-13 14:32:48.028580: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-13 14:32:48.028936: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2021-11-13 14:32:48.029537: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-11-13 14:32:48.029632: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-11-13 14:32:48.029703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.029969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.030169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-11-13 14:32:48.030928: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-13 14:32:48.031476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.031689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2021-11-13 14:32:48.031733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.031955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.032211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2021-11-13 14:32:48.032238: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-11-13 14:32:48.389872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-13 14:32:48.389897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2021-11-13 14:32:48.389902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2021-11-13 14:32:48.390048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.390331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.390557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-13 14:32:48.390765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4704 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65e8f2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4], shape=(1,), dtype=int64)\n",
      "tf.Tensor([8], shape=(1,), dtype=int64)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([4], shape=(1,), dtype=int64)\n",
      "tf.Tensor([19], shape=(1,), dtype=int64)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor([15], shape=(1,), dtype=int64)\n",
      "tf.Tensor([8], shape=(1,), dtype=int64)\n",
      "tf.Tensor([5], shape=(1,), dtype=int64)\n",
      "tf.Tensor([8], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#visualizo algunos datos. Son numeros! \n",
    "for i in dataset.take(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c497d35",
   "metadata": {},
   "source": [
    "Corto el dataset en ventanas, que son las secuencias con las que voy a entrenar. Esto lo hacemos para tener secuencias mas cortas, si entreo a la red con esta secuencia asi como esta, se volveria tan profunda y pesada que seria imposible entrenarla en una PC. Basicamente, lo que hacemos es truncar el proceso de backpropagation en el tiempo. El tamaño de los vectores temporales es un HIPERPARAMETRO MUY IMPORTANTE, porque determina la memoria que tendra este modelo. A menor valor, se entrena mas facil pero tiene menos memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a45d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 100 # largo de cada ventanita\n",
    "window_length = n_steps + 1 # ya que el target es la entrada corrida 1 valor hacia el futuro\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "batch_size = 256\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1,0], windows[:, 1:,0])) # (x, y)\n",
    "\n",
    "# Las entradas categoricas usualmente se codifican, como one-hot o usando embedding. Probar de no hacerlo\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237b1487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-13 14:32:57.303905: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-11-13 14:32:57.322350: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3199980000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 100)\n"
     ]
    }
   ],
   "source": [
    "#visualizo algunos datos. Las entradas son secuencias one-hot, y las salidas secuencias numericas! \n",
    "for i in dataset.take(1):\n",
    "    print(i[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6bfbb",
   "metadata": {},
   "source": [
    "### Modelo:\n",
    "Para hacer las predicciones vamos a armar un modelo RNN usando 2 capas GRU de 128 neuronas, con un 20% de Dropout en las entradas como en los estados ocultos. porque? porque funciona, y es un buen punto inicial. La capa de salida sera una capa Densa distribuida en el tiempo como siempre en estos modelos, que en este caso tenrdra tantas neuronas como caracteres tiene nuestro dataset, ya que la salida es la probabilidad que tiene cada caracter de ser el siguiente. Como la salida de probabilidad debe sumar 1, se aplica la activación softmax a la salida de esta capa densa. Como se predicen probabilidades, se compila con la loss \"sparse_categorical_crossentropy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287bba98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "310/310 [==============================] - 13s 35ms/step - loss: 2.3326\n",
      "Epoch 2/20\n",
      "310/310 [==============================] - 11s 35ms/step - loss: 1.9212\n",
      "Epoch 3/20\n",
      "310/310 [==============================] - 11s 35ms/step - loss: 1.7369\n",
      "Epoch 4/20\n",
      "310/310 [==============================] - 11s 35ms/step - loss: 1.6139\n",
      "Epoch 5/20\n",
      "310/310 [==============================] - 11s 35ms/step - loss: 1.5231\n",
      "Epoch 6/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.4505\n",
      "Epoch 7/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.3954\n",
      "Epoch 8/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.3500\n",
      "Epoch 9/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.3138\n",
      "Epoch 10/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.2825\n",
      "Epoch 11/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.2561\n",
      "Epoch 12/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.2338\n",
      "Epoch 13/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.2149\n",
      "Epoch 14/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.1956\n",
      "Epoch 15/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.1810\n",
      "Epoch 16/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.1668\n",
      "Epoch 17/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.1540\n",
      "Epoch 18/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.1428\n",
      "Epoch 19/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.1305\n",
      "Epoch 20/20\n",
      "310/310 [==============================] - 12s 35ms/step - loss: 1.1220\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tfkl.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2),\n",
    "    tfkl.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    tfkl.TimeDistributed(tfkl.Dense(max_id, activation=\"softmax\"))\n",
    "    ])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707427ca",
   "metadata": {},
   "source": [
    "### Usando el modelo\n",
    "Hay que hacer un pre-procesamiento para usar este modelo. Lo mas simple seria tomar una entrada de texto, hacer el one-hot encoding, pasarlo al modelo para hacer prediccion (el modelo recibe secuencias de cualquier largo!), tomar la salida del modelo (que es un numero, un codigo o ID) y convertirla a caracter a traves del tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "956e7353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "X_new = preprocess([\"Holo es\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687fc66",
   "metadata": {},
   "source": [
    "# Generar Texto!\n",
    "Al igual que en los ejemplos de series numéricas, puedo predecir varios valores futuros yendo de uno a la vez, y sumando este valor al cuerpo de valores de entrada a medida que hago predicciones. En generación de texto esta metodología lleva a repetir palabras u oraciones. Otra alternativa mejor es elegir el caracter siguiente de manera aleatoria, con una probabilidad igual a la estimada por el modelo. Esto genera texto mas diverso e interesante.\n",
    "Para hacer esto, dependemos de la función **tf.random.categorical()**. Esta funcion devuelve muestras random de un cierto numero de clases, dado las probabilidades de cada clase (logits). Para tener mas control sobre la diversidad del texto generado, se pueden dividir los logits por un número denominado \"temperatura\" que se puede ajustar. Con un valor de temperatura = 0, se favorecen los caracteres de alta probabilidad, y con una temperatura cerca de 1 se les da a todos el mismo peso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2859026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_gen(text, n_chars):    \n",
    "    for _ in tqdm(range(n_chars)):\n",
    "        X_new = preprocess([text])\n",
    "        Y_pred = model.predict_classes(X_new)\n",
    "        next_char = tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]\n",
    "        text += next_char\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f323382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:06<00:00, 32.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rio  \n",
      "con la entereza de un gran titan    \n",
      "muero contento hemos batido   \n",
      "al enemigo che capitan   \n",
      "\n",
      "\n",
      "con su figura doblada   \n",
      "vencida como la tarde  \n",
      "\n",
      "\n",
      "por eso es que mi cancion  \n",
      "\n",
      "\n",
      "por eso es que mi ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(simple_gen(\"rio\", 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c3debe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=200, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f85a0c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rio  \n",
      "correntino este donde este  \n",
      "\n",
      "\n",
      "se que tu comprendera  \n",
      "\n",
      "\n",
      "por falos del taragi    \n",
      "solor de habra de llamar   \n",
      "roco en el rostro  \n",
      "de los pobres pescadores  \n",
      "\n",
      "\n",
      "por eso es que mi cancion  \n",
      "\n",
      "\n",
      "por eso e\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"rio \", temperature=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434be80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
